{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem 2.1 \n",
    "To walk “downhill” on the loss function (equation 2.5), we measure its gradient with\n",
    "respect to the parameters ϕ0 and ϕ1. Calculate expressions for the slopes $$∂L/∂ϕ_0$$ and $$∂L/∂ϕ_1$$\n",
    "#### Solution\n",
    "$$L[\\phi]=\\sum_{i=1}^{I} (\\phi_0+\\phi_1x_i-y_i)^2$$\n",
    "The partial derivatives are given by:\n",
    "$$∂L/∂ϕ_0 = \\sum_{i=1}^{I} 2\\cdot (\\phi_0+\\phi_1x_i-y_i)$$\n",
    "$$∂L/∂ϕ_1 = \\sum_{i=1}^{I} 2\\cdot x_i \\cdot (\\phi_0+\\phi_1x_i-y_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2.2 \n",
    "Show that we can find the minimum of the loss function in closed form by setting the expression for the derivatives from problem 2.1 to zero and solving for ϕ0 and ϕ1. Note that\n",
    "this works for linear regression but not for more complex models; this is why we use iterative model fitting methods like gradient descent (figure 2.4)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
